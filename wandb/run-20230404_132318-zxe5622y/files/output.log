WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/10
2023-04-04 13:23:38.963675: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 4
2023-04-04 13:23:38.963921: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2023-04-04 13:23:38.980940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78600 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:17:00.0, compute capability: 8.0
2023-04-04 13:23:38.981892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78600 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:65:00.0, compute capability: 8.0
2023-04-04 13:23:38.982828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78600 MB memory:  -> device: 2, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0
2023-04-04 13:23:38.983760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 78600 MB memory:  -> device: 3, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:e3:00.0, compute capability: 8.0
2023-04-04 13:23:39.012374: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.032ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
WARNING:tensorflow:From /home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
WARNING  From /home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
2023-04-04 13:23:39.280961: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2023-04-04 13:23:45.751937: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
37/37 - 8s - loss: 0.4934 - accuracy: 0.8120 - auc: 0.5434 - precision: 0.2637 - recall: 0.0283 - balanced_accuracy: 0.5053 - val_loss: 0.4588 - val_accuracy: 0.8218 - val_auc: 0.6204 - val_precision: 0.6667 - val_recall: 0.0094 - val_balanced_accuracy: 0.5043
Epoch 2/10
37/37 - 0s - loss: 0.4782 - accuracy: 0.8146 - auc: 0.5857 - precision: 0.3368 - recall: 0.0378 - balanced_accuracy: 0.5103 - val_loss: 0.4359 - val_accuracy: 0.8277 - val_auc: 0.6929 - val_precision: 0.8333 - val_recall: 0.0472 - val_balanced_accuracy: 0.5214
Epoch 3/10
37/37 - 0s - loss: 0.4595 - accuracy: 0.8203 - auc: 0.6270 - precision: 0.4844 - recall: 0.0732 - balanced_accuracy: 0.5280 - val_loss: 0.4192 - val_accuracy: 0.8387 - val_auc: 0.7236 - val_precision: 0.8889 - val_recall: 0.1132 - val_balanced_accuracy: 0.5594
Epoch 4/10
37/37 - 0s - loss: 0.4339 - accuracy: 0.8304 - auc: 0.6832 - precision: 0.6250 - recall: 0.1299 - balanced_accuracy: 0.5561 - val_loss: 0.4080 - val_accuracy: 0.8471 - val_auc: 0.7438 - val_precision: 0.9189 - val_recall: 0.1604 - val_balanced_accuracy: 0.5811
Epoch 5/10
37/37 - 0s - loss: 0.4281 - accuracy: 0.8359 - auc: 0.6935 - precision: 0.6804 - recall: 0.1558 - balanced_accuracy: 0.5704 - val_loss: 0.3986 - val_accuracy: 0.8497 - val_auc: 0.7577 - val_precision: 0.9048 - val_recall: 0.1792 - val_balanced_accuracy: 0.5888
Epoch 6/10
37/37 - 0s - loss: 0.4226 - accuracy: 0.8395 - auc: 0.7048 - precision: 0.7062 - recall: 0.1759 - balanced_accuracy: 0.5799 - val_loss: 0.3908 - val_accuracy: 0.8514 - val_auc: 0.7676 - val_precision: 0.8913 - val_recall: 0.1934 - val_balanced_accuracy: 0.5958
Epoch 7/10
37/37 - 0s - loss: 0.4199 - accuracy: 0.8363 - auc: 0.7079 - precision: 0.6667 - recall: 0.1700 - balanced_accuracy: 0.5754 - val_loss: 0.3841 - val_accuracy: 0.8505 - val_auc: 0.7779 - val_precision: 0.8431 - val_recall: 0.2028 - val_balanced_accuracy: 0.5989
Epoch 8/10
37/37 - 0s - loss: 0.4025 - accuracy: 0.8422 - auc: 0.7449 - precision: 0.7155 - recall: 0.1960 - balanced_accuracy: 0.5861 - val_loss: 0.3784 - val_accuracy: 0.8522 - val_auc: 0.7837 - val_precision: 0.8491 - val_recall: 0.2123 - val_balanced_accuracy: 0.6031
Epoch 9/10
37/37 - 0s - loss: 0.3928 - accuracy: 0.8458 - auc: 0.7588 - precision: 0.7175 - recall: 0.2279 - balanced_accuracy: 0.6045 - val_loss: 0.3740 - val_accuracy: 0.8573 - val_auc: 0.7896 - val_precision: 0.8525 - val_recall: 0.2453 - val_balanced_accuracy: 0.6177
Epoch 10/10
37/37 - 0s - loss: 0.3882 - accuracy: 0.8486 - auc: 0.7664 - precision: 0.7559 - recall: 0.2267 - balanced_accuracy: 0.6050 - val_loss: 0.3694 - val_accuracy: 0.8590 - val_auc: 0.7938 - val_precision: 0.8689 - val_recall: 0.2500 - val_balanced_accuracy: 0.6199
Epoch 00010: val_loss improved from inf to 0.36943, saving model to /home/zadubrov/output/results_train/AR_single-labeled_Fold-0.model.weights.hdf5
INFO     Computation time for training the single-label model for AR: 0.21 min
<IPython.core.display.HTML object>
[ True  True  True ...  True  True  True]
<bound method Model.predict of <keras.engine.sequential.Sequential object at 0x2b35545fb7f0>>
[[0 1 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
<class 'numpy.ndarray'>	4735
<class 'tuple'>	2
WARNING  Using 4735 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.
  0%|                                                                                                                                   | 0/1184 [00:00<?, ?it/s]INFO     num_full_subsets = 0
INFO     remaining_weight_vector = [0.12197261 0.06101611 0.0406973  ... 0.00023811 0.00023811 0.00023811]
INFO     num_paired_subset_sizes = 1023
INFO     weight_left = 1.0000000000000002
INFO     np.sum(w_aug) = 2048.0
INFO     np.sum(self.kernelWeights) = 1.0000000000000004
The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:
from sklearn.pipeline import make_pipeline
model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())
If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:
kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)
Set parameter alpha to: original_alpha * np.sqrt(n_samples).
Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.541e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.292e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=7.124e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=7.089e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 48 iterations, alpha=7.114e-04, previous alpha=7.061e-04, with an active set of 19 regressors.
  0%|                                                                                                                                   | 0/1184 [03:36<?, ?it/s]
Traceback (most recent call last):
  File "/home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs0/home/zadubrov/deepFPlearn_exai/dfpl/__main__.py", line 415, in <module>
    main()
  File "/gpfs0/home/zadubrov/deepFPlearn_exai/dfpl/__main__.py", line 392, in main
    train(fixed_opts)
  File "/gpfs0/home/zadubrov/deepFPlearn_exai/dfpl/__main__.py", line 190, in train
    sl.train_single_label_models(df=df, opts=opts)
  File "/gpfs0/home/zadubrov/deepFPlearn_exai/dfpl/single_label_model.py", line 551, in train_single_label_models
    performance = fit_and_evaluate_model(x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test,
  File "/gpfs0/home/zadubrov/deepFPlearn_exai/dfpl/single_label_model.py", line 422, in fit_and_evaluate_model
    sd.shap_explain(x_train, x_test, model, target, opts.outputDir, drop_values=True, threshold=10, save_values=True)
  File "/gpfs0/home/zadubrov/deepFPlearn_exai/dfpl/shap_dfpl.py", line 27, in shap_explain
    shap_values, indices = explainer.shap_values(x_test, nsamples=100)
  File "/home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/site-packages/shap/explainers/_kernel.py", line 190, in shap_values
    explanations.append(self.explain(data, **kwargs))
  File "/home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/site-packages/shap/explainers/_kernel.py", line 388, in explain
    vphi, vphi_var = self.solve(self.nsamples / self.max_samples, d)
  File "/home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/site-packages/shap/explainers/_kernel.py", line 565, in solve
    nonzero_inds = np.nonzero(LassoLarsIC(criterion=c).fit(mask_aug, eyAdj_aug).coef_)[0]
  File "/home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py", line 2229, in fit
    self.noise_variance_ = self._estimate_noise_variance(
  File "/home/zadubrov/.conda/envs/dfpl_exai/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py", line 2269, in _estimate_noise_variance
    raise ValueError(
ValueError: You are using LassoLarsIC in the case where the number of samples is smaller than the number of features. In this setting, getting a good estimate for the variance of the noise is not possible. Provide an estimate of the noise variance in the constructor.